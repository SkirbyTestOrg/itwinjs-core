To Do (core display performance testing):
* Generalize the test fixture such that its input is a json representation of
  a set of test cases, each test case specifying iModel (and location thereof
  - which could be iModelHub or file system), view, view flags, render mode,
    etc, each to be executed and measured in turn.
* Obtain a small diverse set of good test iModels which will exercise various
  display features - e.g. transparency, materials, etc - including some larger
  iModels. Make these accessible via iModelHub.
* Supply a default configuration json file which exercises this set of
  iModels.
* Output something like csv or json, not xlsx.
* Ensure presence of .Tiles caches do not influence measurements.
* Replace hard-coded integer constant 'frame indices' with a mapping of
  'operation name' (e.g. 'render opaque', 'composite', etc) and timing data.
  The operation names become the keys in your output json (or column names if
  you end up outputting csv).
* Rather than attempting to save a .png image as a file on local disk, which
  requires user interaction, consider storing it in your results output e.g. as
  base64-encoded binary data.
* Consolidate output data to include useful summary data and exclude noise -
  e.g., containing only averages of multiple runs, possibly also min and max
  values, etc.
* Remove console spam.
* Ensure tests can be executed outside of electron. Otherwise they will be
  useless for testing performance in browsers other than Chrome and on mobile
  devices.

Further work (beyond core display performance testing):
* Measure time required to generate tiles on backend.
* Measure time required to convert binary tiles obtained from backend into
  graphics.
* Execute tests on variety of devices and browsers; collect and analyze
  results; file enhancements to address specific performance issues
  discovered.

Finished:
*Make sure timing data being collected is accurate - done; data collected now matches data collected while running continuous rendering in the simpleview test app.
*Figure out why gl.Finish isn't working and/or how to test the amount of time the gpu spends when doing a drawFrame - done; gl.Finish just doesn't seem to work, so instead we are reading back 1 pixel, which forces the gpu to completely finish all of the currently queued commands before reading the 1 pixel (thereby allowing us to see how much time the gpu spends finishing executing everything in the queue).
*Figure out a way to save a png of what's getting rendered - done; however, needs to be better - currently good enough to verify that things are running correctly, but isn't good enough to run a test that saves multiple pngs without the user manually clicking the save button.
*Wait until the all the tiles are fully loaded before continuing with the test - done.

How to use the current performance tests:
*Currently, I’ve committed what I’ve got in a separate branch called display-perf-testing.
*To run what’s currently there, just run the command “npm run test:frontend:performance” to run all of the performance tests (‘all’ right now is just the startup test and the PerformanceTests.test.ts).
*Right now, to change what model or view you want to use, you need to change the default configuration settings. Change “Wraith_MultiMulti.ibim” to whatever ibim file you wish to open. It will look in the test-apps/testbed/frontend/performance/imodels folder to find the imodel, or you can hardcode the imodel path to iModelName. Similarly, you can change the view by giving a different value to viewName.
  configuration = {
    userName: "bistroDEV_pmadm1@mailinator.com",
    password: "pmadm1",
    iModelName: path.join(iModelLocation, "Wraith_MultiMulti.ibim"), // "D:\\models\\ibim_bim0200dev\\Wraith.ibim", // "D:\\models\\ibim_bim0200dev\\Wraith.ibim", // "atp_10K.bim", // "D:/models/ibim_bim0200dev/Wraith.ibim", // "atp_10K.bim",
    viewName: "V0", // "Physical-Tag",
  } as SVTConfiguration;
*To save a png of what’s being run, just add a savePng() function call in the PerformanceTests.test.ts file, and then you will have to hit the save button when a popup comes up, and it’ll let you choose the file name and location at that time as well. You may need to uncomment the savePng() function definition if it is currently commented out.
*The current timings will also get saved automatically to the performanceResults.xlsx file.
